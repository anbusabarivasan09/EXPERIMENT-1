{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anbusabarivasan09/EXPERIMENT-1/blob/main/data_science_assignment_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc537aGkiQqm"
      },
      "source": [
        "## Q1. Rolling Sensor Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I1fnnkRXkTHp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IexnRJdUiQqn"
      },
      "outputs": [],
      "source": [
        "print('Anbusabarivasan M')\n",
        "print('24BAD009')\n",
        "print()\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "sensor = np.array([22.1, 22.5, 23.0, 24.8, 25.1, 26.0, 27.5, 28.0])\n",
        "\n",
        "sensor_series = pd.Series(sensor)\n",
        "rolling_mean = sensor_series.rolling(window=3).mean()\n",
        "print('Rolling Mean (window=3):')\n",
        "print(rolling_mean)\n",
        "print()\n",
        "\n",
        "rolling_std = sensor_series.rolling(window=3).std()\n",
        "print('Rolling Standard Deviation (window=3):')\n",
        "print(rolling_std)\n",
        "print()\n",
        "\n",
        "overall_std = np.std(sensor)\n",
        "threshold = 1.5 * overall_std\n",
        "anomalies = sensor[np.abs(sensor - np.mean(sensor)) > threshold]\n",
        "print(f'Overall Standard Deviation: {overall_std}')\n",
        "print(f'Threshold (1.5 * std): {threshold}')\n",
        "print(f'Anomalous Values: {anomalies}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7HY2W0aiQqn"
      },
      "source": [
        "## Q2. Distance Matrix Computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9IHzXwviQqo"
      },
      "outputs": [],
      "source": [
        "print('Anbusabarivasan M')\n",
        "print('24BAD009')\n",
        "print()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "points = np.array([[2, 3], [5, 4], [8, 7], [1, 6]])\n",
        "\n",
        "diff = points[:, np.newaxis, :] - points[np.newaxis, :, :]\n",
        "distance_matrix = np.sqrt(np.sum(diff**2, axis=2))\n",
        "\n",
        "print('Points:')\n",
        "print(points)\n",
        "print()\n",
        "print('Euclidean Distance Matrix:')\n",
        "print(distance_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNzXEwyZiQqo"
      },
      "source": [
        "## Q3. Monte Carlo Risk Estimation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9PgaSHRiQqo"
      },
      "outputs": [],
      "source": [
        "print('Anbusabarivasan M')\n",
        "print('24BAD009')\n",
        "print()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(10)\n",
        "returns = np.random.normal(0.001, 0.02, 10000)\n",
        "\n",
        "expected_return = np.mean(returns)\n",
        "print(f'Expected Return: {expected_return}')\n",
        "print()\n",
        "\n",
        "probability_of_loss = np.sum(returns < 0) / len(returns)\n",
        "print(f'Probability of Loss: {probability_of_loss}')\n",
        "print()\n",
        "\n",
        "confidence_interval = np.percentile(returns, [2.5, 97.5])\n",
        "print(f'95% Confidence Interval: [{confidence_interval[0]}, {confidence_interval[1]}]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENbirWD0iQqp"
      },
      "source": [
        "## Q4. Sparse Interaction Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1X238f_iQqp"
      },
      "outputs": [],
      "source": [
        "print('Anbusabarivasan M')\n",
        "print('24BAD009')\n",
        "print()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "interactions = np.array([[0, 0, 5, 0], [1, 0, 0, 0], [0, 0, 0, 3], [4, 0, 0, 0]])\n",
        "\n",
        "total_elements = interactions.size\n",
        "zero_elements = np.sum(interactions == 0)\n",
        "sparsity_percentage = (zero_elements / total_elements) * 100\n",
        "print('Interaction Matrix:')\n",
        "print(interactions)\n",
        "print()\n",
        "print(f'Sparsity Percentage: {sparsity_percentage}%')\n",
        "print()\n",
        "\n",
        "row_sums = np.sum(interactions, axis=1, keepdims=True)\n",
        "row_sums[row_sums == 0] = 1\n",
        "normalized_matrix = interactions / row_sums\n",
        "print('Normalized Rows:')\n",
        "print(normalized_matrix)\n",
        "print()\n",
        "\n",
        "top_interactions = np.max(interactions, axis=1)\n",
        "print('Top Interaction for Each Row:')\n",
        "print(top_interactions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pi8q1R-iQqp"
      },
      "source": [
        "## Q5. Student Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBYHFG6aiQqp"
      },
      "outputs": [],
      "source": [
        "print('Anbusabarivasan M')\n",
        "print('24BAD009')\n",
        "print()\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    \"Student\": [\"A\", \"B\", \"C\", \"D\", \"E\"],\n",
        "    \"Math\": [78, 85, 67, 90, 72],\n",
        "    \"Physics\": [74, 88, 69, 92, 70],\n",
        "    \"Attendance\": [82, 90, 76, 95, 80]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print('Student Data:')\n",
        "print(df)\n",
        "print()\n",
        "\n",
        "df['Average'] = df[['Math', 'Physics', 'Attendance']].mean(axis=1)\n",
        "print('Average Scores:')\n",
        "print(df[['Student', 'Average']])\n",
        "print()\n",
        "\n",
        "def assign_grade(avg):\n",
        "    if avg >= 90:\n",
        "        return 'A'\n",
        "    elif avg >= 80:\n",
        "        return 'B'\n",
        "    elif avg >= 70:\n",
        "        return 'C'\n",
        "    elif avg >= 60:\n",
        "        return 'D'\n",
        "    else:\n",
        "        return 'F'\n",
        "\n",
        "df['Grade'] = df['Average'].apply(assign_grade)\n",
        "print('Grades:')\n",
        "print(df[['Student', 'Average', 'Grade']])\n",
        "print()\n",
        "\n",
        "top_performer = df.loc[df['Average'].idxmax(), 'Student']\n",
        "print(f'Top Performer: Student {top_performer}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRTmnsm4iQqp"
      },
      "source": [
        "## Q6. Time-Series Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps5xqPAniQqq"
      },
      "outputs": [],
      "source": [
        "print('Anbusabarivasan M')\n",
        "print('24BAD009')\n",
        "print()\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "timestamps = pd.date_range(\"2024-01-01\", periods=10, freq=\"6H\")\n",
        "values = [12, 18, 20, 22, 19, 15, 14, 17, 21, 23]\n",
        "\n",
        "df = pd.DataFrame({'timestamp': timestamps, 'value': values})\n",
        "\n",
        "df['hour'] = df['timestamp'].dt.hour\n",
        "df['day'] = df['timestamp'].dt.day\n",
        "print('Extracted Hour and Day:')\n",
        "print(df[['timestamp', 'hour', 'day', 'value']])\n",
        "print()\n",
        "\n",
        "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "print('Cyclic Features:')\n",
        "print(df[['hour', 'hour_sin', 'hour_cos']])\n",
        "print()\n",
        "\n",
        "peak_hour = df.loc[df['value'].idxmax(), 'hour']\n",
        "print(f'Hour with Peak Activity: {peak_hour}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScrOd_xxiQqq"
      },
      "source": [
        "## Q7. Data Leakage Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNN5cbiPiQqq"
      },
      "outputs": [],
      "source": [
        "print('Anbusabarivasan M')\n",
        "print('24BAD009')\n",
        "print()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "train = pd.DataFrame({\n",
        "    \"id\": [1, 2, 3, 4],\n",
        "    \"score\": [65, 70, 80, 90],\n",
        "    \"result\": [0, 0, 1, 1]\n",
        "})\n",
        "\n",
        "test = pd.DataFrame({\n",
        "    \"id\": [3, 5],\n",
        "    \"score\": [80, 75],\n",
        "    \"result\": [1, 0]\n",
        "})\n",
        "\n",
        "print('Training Data:')\n",
        "print(train)\n",
        "print()\n",
        "print('Testing Data:')\n",
        "print(test)\n",
        "print()\n",
        "\n",
        "overlapping_ids = set(train['id']).intersection(set(test['id']))\n",
        "print(f'Overlapping IDs: {overlapping_ids}')\n",
        "print()\n",
        "\n",
        "leakage_columns = []\n",
        "if 'result' in test.columns:\n",
        "    leakage_columns.append('result')\n",
        "print(f'Columns that may cause leakage: {leakage_columns}')\n",
        "print()\n",
        "\n",
        "leakage_percentage = (len(overlapping_ids) / len(test)) * 100\n",
        "print(f'Percentage of Leakage: {leakage_percentage}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoXgVst5iQqr"
      },
      "source": [
        "## Q8. Memory Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDLDNkiziQqr"
      },
      "outputs": [],
      "source": [
        "print('Anbusabarivasan M')\n",
        "print('24BAD009')\n",
        "print()\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"A\": np.random.randint(0, 1000, 10000),\n",
        "    \"B\": np.random.rand(10000),\n",
        "    \"C\": [\"category\"] * 10000\n",
        "})\n",
        "\n",
        "memory_before = df.memory_usage(deep=True).sum()\n",
        "print(f'Memory Usage Before Optimization: {memory_before} bytes')\n",
        "print(f'Memory Usage Before Optimization: {memory_before / 1024:.2f} KB')\n",
        "print()\n",
        "\n",
        "df['A'] = pd.to_numeric(df['A'], downcast='integer')\n",
        "df['B'] = pd.to_numeric(df['B'], downcast='float')\n",
        "df['C'] = df['C'].astype('category')\n",
        "\n",
        "memory_after = df.memory_usage(deep=True).sum()\n",
        "print(f'Memory Usage After Optimization: {memory_after} bytes')\n",
        "print(f'Memory Usage After Optimization: {memory_after / 1024:.2f} KB')\n",
        "print()\n",
        "\n",
        "reduction = ((memory_before - memory_after) / memory_before) * 100\n",
        "print(f'Memory Reduction: {reduction:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJstFB_tiQqr"
      },
      "source": [
        "## Q9. Uncertainty Visualization (Matplotlib)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IilJlynUiQqr"
      },
      "outputs": [],
      "source": [
        "print('Anbusabarivasan M')\n",
        "print('24BAD009')\n",
        "print()\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.arange(1, 11)\n",
        "y = np.array([10, 12, 15, 18, 20, 22, 24, 23, 25, 27])\n",
        "error = np.array([1, 1.2, 1.5, 1.3, 1.4, 1.6, 1.8, 1.7, 1.9, 2.0])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, y, 'b-', linewidth=2, label='Data')\n",
        "plt.fill_between(x, y - error, y + error, alpha=0.3, color='blue', label='Uncertainty')\n",
        "\n",
        "max_error_idx = np.argmax(error)\n",
        "plt.plot(x[max_error_idx], y[max_error_idx], 'ro', markersize=10)\n",
        "plt.annotate(f'Max Uncertainty\\n({x[max_error_idx]}, {y[max_error_idx]})',\n",
        "             xy=(x[max_error_idx], y[max_error_idx]),\n",
        "             xytext=(x[max_error_idx] + 1, y[max_error_idx] + 2),\n",
        "             arrowprops=dict(arrowstyle='->', color='red'),\n",
        "             fontsize=10, color='red')\n",
        "\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Line Graph with Uncertainty')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f'Point of Maximum Uncertainty: x={x[max_error_idx]}, y={y[max_error_idx]}, error={error[max_error_idx]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz7l7AXGiQqr"
      },
      "source": [
        "## Q10. Correlation Heatmap (Matplotlib only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mdAkK9NiQqr"
      },
      "outputs": [],
      "source": [
        "print('Anbusabarivasan M')\n",
        "print('24BAD009')\n",
        "print()\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"Temp\": [30, 32, 34, 36, 38],\n",
        "    \"Humidity\": [70, 65, 60, 55, 50],\n",
        "    \"Pressure\": [1012, 1010, 1008, 1006, 1004]\n",
        "})\n",
        "\n",
        "print('DataFrame:')\n",
        "print(df)\n",
        "print()\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "print('Correlation Matrix:')\n",
        "print(correlation_matrix)\n",
        "print()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')\n",
        "\n",
        "ax.set_xticks(np.arange(len(correlation_matrix.columns)))\n",
        "ax.set_yticks(np.arange(len(correlation_matrix.columns)))\n",
        "ax.set_xticklabels(correlation_matrix.columns)\n",
        "ax.set_yticklabels(correlation_matrix.columns)\n",
        "\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(len(correlation_matrix.columns)):\n",
        "        text = ax.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',\n",
        "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n",
        "\n",
        "ax.set_title('Correlation Heatmap')\n",
        "fig.colorbar(im, ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}